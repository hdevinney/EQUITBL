#!/usr/bin/python3
# -*- coding: utf-8 -*-

# version: 1.1 (21.09.2021)
# author: Hannah Devinney
# loads an existing, non-equitbl-compatible corpus and converts to equitbl-compatible json based on user-provided directions to the text
# updated in version 1.1: incremental storage of dataframes (pickled). This both saves progress in case of interruption/failure, and speeds dataframe concatentation

import pandas as pd
import io
import os
import pickle

#CONSTANTS
TEXT_FIELD = "text"
ID_FIELD = "doc_id"

#takes path to original file, names of text
#file, text and id paths should be STRINGS (text_path may be list of strings)
def map_to_schema(original_file, text_path, id_path="", merge_texts=True, id_preamble=""):
    '''Takes a json file; names for text (string or list of strings) and document id entries; returns a dataframe compatible with equitbl's expected schema. Set merge_texts to false if there are multiple text entries per document and you would like them to be counted separately.'''
   
    df_in = pd.read_json(open(original_file), encoding="utf-8")
    #make sure each row has an ID we can consistently find
    if id_path:
        df_in = df_in.rename(columns={id_path:ID_FIELD})
    else:
        df_in.reset_index(inplace=True)
        df_in = df_in.rename(columns={'index':ID_FIELD})
    
    #deal with multiple text entries/document
    if isinstance(text_path, list):
        if merge_texts is True:
            #concatenate texts under same doc_ID (default behavior)
            df_out = df_in[[ID_FIELD]]
            df_out[TEXT_FIELD] = '' #add empty string, fill with all texts
            for path in text_path:
                df_out[TEXT_FIELD] = df_out[TEXT_FIELD].astype(str) + df_in[path].astype(str)
                #just checking...
                print(df_in[path].astype(str))
        else:
            #split into sub-documents
            df_out = pd.DataFrame(columns=[ID_FIELD, TEXT_FIELD])

            #create temp dfs for every text path, then add them to df_out
            for path in text_path:
                df_tmp = df_in[[ID_FIELD, path]]
                df_tmp = df_tmp.rename(columns={path:TEXT_FIELD})
                #re-name IDs
                df_tmp[ID_FIELD] = df_tmp[ID_FIELD].astype(str) + "_" + path
                df_out = df_out.append(df_tmp, ignore_index=True)
    #deal with single text entry/document
    else:
        #normal
        df_out = df_in[[ID_FIELD, text_path]]
        df_out = df_out.rename(columns={text_path:TEXT_FIELD})

    return df_out

    
#load user mapping for document IDs
# (and figure out default system for if no IDs)





def chunk_by_sentence(original_df, df_storage_path, sentence_regex=r"[?!\.]\W"):
    '''Takes a dataframe as generated by corpus_mapping.map_to_schema, sub-divides the texts into sentences (returns new dataframe). The default sentence regex is simply major delimiters (. ? and !) followed by a non-alphanumeric character: this over-captures and can be replaced with something more custom if desired.
    '''

    #formatting...
    if df_storage_path[-1] != "/":
        df_storage_path = df_storage_path + "/"
    pkl_regex = re.compile("tmpdf([\d]+)\.pkl")
        
    #ensure df_storage_path exists
    check = os.path.isdir(df_storage_path)
    if not check:
        os.makedirs(df_storage_path)
        print("Temporary storage is in {}".format(df_storage_path))

    #find any "tmpdf" files (no need to process them again)
    dir_contents = os.scandir(df_storage_path)
    files = [os.path.isfile(os.path.join(df_storage_path, f)) for f in dir_contents]
    biggest_i = 0
    for f in files:
        match = pkl_regex.match(f)
        if match:
            i = match.group(1)
            if i > biggest_i:
                biggest_i = i

    if biggest_i > 0:
        #slice original_df so we're only dealing with the un-processed part
        print("TODO: don't start over from the beginning")
        

    new_df = pd.DataFrame(columns=[ID_FIELD, TEXT_FIELD])
    for i in original_df.index:
        if i % 1000 == 0:
            print(i)
            #save and reset new_df
            save_spot = df_storage_path + "tmpdf" + str(i) + ".pkl"
            new_df.to_pickle(save_spot)
            del new_df
            new_df = pd.DataFrame(columns=[ID_FIELD, TEXT_FIELD])
            
        current_id = original_df[ID_FIELD][i]
        current_text = original_df[TEXT_FIELD][i]

        chunks = re.split(sentence_regex, current_text)
        new_ids = [str(current_id) + str(j) for j in size(chunks)]

        #these might need to be list(zip(new_ids, chunks)) instead of [] notation
        tmp_df = pd.DataFrame([new_ids, chunks], columns=[ID_FIELD, TEXT_FIELD])

        new_df.append(tmp_df)

    #get all the new_dfs we pickled
    newest_df = pd.DataFrame(columns=[ID_FIELD, TEXT_FIELD])
    
    for fname in os.listdir(df_storage_path):
        path = os.path.join(df_storage_path, fname)
        with open(path, 'r') as f:
            if pkl_regex.match(file): #only open the right kid of file...
                #add the unpickled dataframe onto our big final  dataframe
                newest_df.append(pd.read_pickle(path))

    

    return newest_df

        
def chunk_text(words, n):
    '''splits a list of words into chunks of size n'''
    for i in range(0, len(words), n):
        yield words[i:i+n]


def chunk_by_size(original_df, output_path, doc_size=15):
    '''Takes a dataframe as generated by corpus_mapping.map_to_schema, sub-divides the texts into chunks of a size doc_size. The result is saved to the .json file specified in output_path. Chunks are non-overlapping, and if a document doesn't divide evently the final chunk will be the 'remainder' of size <= doc_size.
    '''
    if doc_size <= 0:
        #THROW AN ERROR?
        #currently just does nothing
        return original_df

    new_df = pd.DataFrame(columns=[ID_FIELD, TEXT_FIELD])
    for i in original_df.index:
        if i % 100 == 0:
            print(i)
            new_df.reset_index(drop=True, inplace=True)
            #save periodically
            new_df.to_json(output_path, force_ascii=False)

        current_id = original_df[ID_FIELD][i]
        current_text = original_df[TEXT_FIELD][i]
        words = current_text.split()
        chunk_lists = list(chunk_text(words, doc_size))

        chunks = [' '.join(l) for l in chunk_lists]
        new_ids = [str(current_id) + str(j) for j in range(len(chunks))]     
        
        #these might need to be list(zip(new_ids, chunks)) instead of [] notation
        ## tmp_df = pd.DataFrame([new_ids, chunks], columns=[ID_FIELD, TEXT_FIELD])
        tmp_df = pd.DataFrame({ID_FIELD: new_ids, TEXT_FIELD: chunks})
        
        new_df = new_df.append(tmp_df)
    

#    return new_df


