#!/usr/bin/python3
# -*- coding: utf-8 -*-

# version: 1.0 (10.05.2021)
# author: Hannah Devinney
# toolkit for preprocessing ENGLISH text (for use in topic modeling)
# requirements: nltk, numpy, gensim, pandas, re

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

from tools.corpus import corpus_mapping
#import corpus_mapping

import pandas as pd
import numpy as np
import re
import math
import gensim
from gensim import corpora
from collections import defaultdict

#NB I actually don't think we should use this function but I wrote it first so it's still here
def normalize_all(df):
    '''Takes a dataframe as generated by corpus_mapping.map_to_schema and normalizes all of the TEXT_FIELDS. TODO: add options for what to normalize/eliminate'''
    for i in df.index:
        text = df[corpus_mapping.TEXT_FIELD][i]
        #basic formatting
        text = text.replace('\n',' ').replace('\t',' ').replace("&"," and ")
        #appostrophes are unicode hell
        text = text.replace(u"\u2019", "'").replace(u"\u02B9", "'").replace(u"\u02BC", "'").replace(u"\u2032", "'").replace(u"\u02C8", "'").replace(u"\u0301", "'").replace(u"\u05F3", "'").replace(u"\u275C", "'")
        #hyphens we want to "keep"
        text = text.replace ('non-binary','nonbinary')

        #get rid of everything but alphonumeric, space, and apostrophe
        text = re.sub(r"[^\w ']", ' ', text, flags=re.UNICODE)

        df[corpus_mapping.TEXT_FIELD][i] = text

def normalize_text(text):
    '''Takes a string and returns its normalized form'''
    #basic formatting
    text = text.replace('\n',' ').replace('\t',' ').replace("&"," and ")
    #appostrophes are unicode hell
    text = text.replace(u"\u2019", "'").replace(u"\u02B9", "'").replace(u"\u02BC", "'").replace(u"\u2032", "'").replace(u"\u02C8", "'").replace(u"\u0301", "'").replace(u"\u05F3", "'").replace(u"\u275C", "'")
    #hyphens we want to "keep"
    text = text.replace ('non-binary','nonbinary')

    #get rid of everything but alphonumeric, space, and apostrophe
    return re.sub(r"[^\w ']", ' ', text, flags=re.UNICODE)

def get_docs_and_frequencies(lemma_dictionary):
    '''Takes a dictionary (output of get_*_dictionary function), returns the documents as a list of lists (of tokens) and a defaultdict of frequency counts. '''
    documents = []
    frequency = defaultdict(int)
    for key in lemma_dictionary:
        doc = lemma_dictionary[key]
        documents.append(doc)
        for token in doc:
            frequency[token] += 1

    return documents, frequency

def prune_dict(orig_dict, frequency, seed_words, min_thresh):
    '''Takes a list of lists (tokens in documents), a defaultdict(int) of frequency info, a list of terms to 'protect' (i.e. which will NOT be pruned regardless of frequency), and a minimum number-of-occurences threshold. Returns a list of lists'''
    documents = [
        [t for t in doc if (t in seed_words) or (frequency[t] > min_thresh)]
        for doc in orig_dict
    ]
    return documents
    
def get_tokens_dictionary(df, stopwords=[]): #just
    '''Takes a dataframe as generated by corpus_mapping.map_to_schema and creates a dictionary of tokens (not lemmatized or POS tagged) for Topic Modeling. Optional: specify a list of stopwords to ignore'''
    dictionary = {}
    for i in df.index:
        #track progress
        if i % 100 == 0:
            print(i)

        text_id = df[corpus_mapping.ID_FIELD][i]
        text = df[corpus_mapping.TEXT_FIELD][i]

        #normalize
        text = normalize_text(text)

        #get tokens (list of str)
        all_tokens = nltk.word_tokenize(text)

        #clean up: remove single-letter words (e.g. broken acronyms like u.s.a.)
        #TODO: STOPWORDS
        tokens = [w for w in all_tokens if len(w) > 1 and w not in stopwords]

        if tokens: #make sure not empty
            dictionary[text_id] = tokens

    return dictionary
        

def get_pos_lemmas_dictionary(df, ignore_tags = [], stopwords = []): #get POS-tagged lemmas
    '''Takes a dataframe as generated by corpus_mapping.map_to_schema and creates a dictionary of lemmaPOS tokens (lemmatized and POS tagged) for Topic Modeling. Removes any tokens with a POS tag indicated in ignore_tags. Optional: specify a list of stopwords to remove'''
    dictionary = {}
    for i in df.index:
        #track progress
        if i % 100 == 0:
            print(i)

        text_id = df[corpus_mapping.ID_FIELD][i]
        text = df[corpus_mapping.TEXT_FIELD][i]

        #normalize
        text = normalize_text(text)

        #get tokens (list of str)
        tokens = nltk.word_tokenize(text)

        #POS tagging -- Penn Treebank (by default in nltk)
        all_tagged = nltk.pos_tag(tokens)

        #filter out single-letters and anything in ignore_tags
        tagged = [w for w in all_tagged if (w[1] not in ignore_tags) and (w[0] not in stopwords) and len(w[0]) > 1]

        #lemmatize and attach POS information
        lemmas = get_disambiguated_lemmas(tagged)

        if lemmas: #make sure not empty
            dictionary[text_id] = lemmas

    return dictionary

def get_chunked_pos_lemmas_dictionary(df, ignore_tags = [], stopwords=[], chunk_size=24): #get POS-tagged lemmas
    '''Takes a dataframe as generated by corpus_mapping.map_to_schema and creates a dictionary of lemmaPOS tokens (lemmatized and POS tagged), where every document is composed of an equal number (chunk_size) of tokens, for Topic Modeling. Chunks overlap by a sliding "window" of half the chunk_size (default chunk_size is 24, so default window is 12). Removes any tokens with a POS tag indicated in ignore_tags and any words appearing in stopwords.'''
    dictionary = {}
    for i in df.index:
        #track progress
        if i % 100 == 0:
            print(i)

        text_id = df[corpus_mapping.ID_FIELD][i]
        text = df[corpus_mapping.TEXT_FIELD][i]

        #normalize
        text = normalize_text(text)

        #get tokens (list of str)
        tokens = nltk.word_tokenize(text)

        #POS tagging -- Penn Treebank (by default in nltk)
        all_tagged = nltk.pos_tag(tokens)

        #filter out single-letters and anything in ignore_tags
        #TODO: ADD STOPWORD FILTERING!
        tagged = [w for w in all_tagged if (w[1] not in ignore_tags)and (w[0] not in stopwords) and len(w[0]) > 1]

        #lemmatize and attach POS information
        lemmas = get_disambiguated_lemmas(tagged)

        if lemmas: #make sure not empty
            #chunk 'em
            chunks = chunk_lemmas(lemmas, chunk_size)
            #invent some unique index for every chunk, add to dictionary
            j = 0
            for chunk in chunks:
                doc_id = str(text_id) + str(j)
                dictionary[doc_id] = chunk
                j += 1

    return dictionary

def get_chunked_tokens_dictionary(df, chunk_size=24): #get POS-tagged lemmas
    '''Takes a dataframe as generated by corpus_mapping.map_to_schema and creates a dictionary of tokens, where every document is composed of an equal number (chunk_size) of tokens, for Topic Modeling. Chunks overlap by a sliding "window" of half the chunk_size (default chunk_size is 24, so default window is 12). Removes any tokens with a POS tag indicated in ignore_tags and any words appearing in stopwords.'''
    dictionary = {}
    for i in df.index:
        #track progress
        if i % 100 == 0:
            print(i)

        text_id = df[corpus_mapping.ID_FIELD][i]
        #(text is ALREADY normalized/lemmatized/POS-tagged in this case)
        token_string = df[corpus_mapping.TEXT_FIELD][i]

        #get tokens (list of str)
        tokens = token_string.split(" ")

        if tokens: #make sure not empty
            #chunk 'em
            chunks = chunk_lemmas(tokens, chunk_size)
            #invent some unique index for every chunk, add to dictionary
            j = 0
            for chunk in chunks:
                doc_id = str(text_id) + str(j)
                dictionary[doc_id] = chunk
                j += 1

    return dictionary


def chunk_lemmas(lemmas, chunk_size):
    '''Deals with splitting a list of lemmas into evenly-sized chunks of lemmas, following a sliding window (half of the chunk_size, rounded down). The final chunk is "pushed" to the end to maintain the standard size'''
#    window = int(math.floor(chunk_size/2))    #PYTHON 2 returns a float from floor??
    window = math.floor(chunk_size/2)          #PYTHON 3 returns an int
    chunks = []
    if len(lemmas) <= chunk_size:
           return [lemmas] #no need to chunk

    i = 0 #indexing
    while i < (len(lemmas) - chunk_size):
           chunk = lemmas[i:i+chunk_size]
           chunks.append(chunk)
           i += window #THIS NEEDS TO BE AN INT for slicing to work!

    #final chunk: start from end!
    chunk = lemmas[len(lemmas)-chunk_size:len(lemmas)]
    chunks.append(chunk)

    return chunks



def map_to_wordnet_pos(tag):
    '''Takes a Penn Treebank POS tag and maps it to the apprporiate wordnet tag (based on first letter). This information is only used to support the lemmatizer.'''
    if tag[0] == 'J':
        return wordnet.ADJ
    elif tag[0] == 'V':
        return wordnet.VERB
    elif tag[0] == 'N':
        return wordnet.NOUN
    elif tag[0] == 'R':
        return wordnet.ADV
    else:
        return ''
        

def get_disambiguated_lemmas(words, simplify_tags=True):
    '''Takes a list of tuples (tokens and POS information in Penn Treebank format). Lemmatizes the tokens, and returns a list of lemmaPOS strings. Set simplify_tags to False in order to return given POS tags (simplify_tags True merges singular and plural nouns and adjectives; simple, comparative, and superlative adjectives; and all verb forms)'''
    disambiguated = []
    lemmatizer = WordNetLemmatizer()
    for w in words:
        word = w[0].lower()
        pos = w[1]
        wordnet_pos = map_to_wordnet_pos(pos)
        if wordnet_pos:
            lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)
        else:
            lemma = lemmatizer.lemmatize(word)
            
        #handle neopronouns (over-captures, but also: it's so rare to find them that this doesn't seem to matter... :C)
        if word is ("ze" or "xe" or "xem"): #nom/acc
            newword = word + "PRP"
        if word is ("hir" or "xyr"): #possessive (hir is also acc but oh well, "her" is the same way)
            newword = word + "PRP$"

        if simplify_tags:
            if pos == "NNS": #merge sing and pl common nouns
                newword = lemma + "NN"
            elif pos == "NNPS": #merge sing and pl proper nouns
                newword = lemma + "NNP"
            elif pos == "JJR" or pos == "JJS": #merge adjective forms
                newword = lemma + "JJ"
            elif pos in ["VBD", "VBG", "VBN", "VBP", "VBZ"]: #verbs
                newword = lemma + "VB"
            else:
                newword = lemma + pos

            disambiguated.append(newword)
    return disambiguated


    
def convert_pos_lemmas_df(df, ignore_tags = [], stopwords = []): #get POS-tagged lemmas
    '''Takes a dataframe as generated by corpus_mapping.map_to_schema and converts the text field to lemmaPOS tokens (lemmatized and POS tagged). Removes any tokens with a POS tag indicated in ignore_tags. Optional: specify a list of stopwords to remove'''
    for i in df.index:
        #track progress
        if i % 100 == 0:
            print(i)

        text_id = df[corpus_mapping.ID_FIELD][i]
        text = df[corpus_mapping.TEXT_FIELD][i]

        #normalize
        text = normalize_text(text)

        #get tokens (list of str)
        tokens = nltk.word_tokenize(text)

        #POS tagging -- Penn Treebank (by default in nltk)
        all_tagged = nltk.pos_tag(tokens)

        #filter out single-letters and anything in ignore_tags
        tagged = [w for w in all_tagged if (w[1] not in ignore_tags) and (w[0] not in stopwords) and len(w[0]) > 1]

        #lemmatize and attach POS information
        lemmas = get_disambiguated_lemmas(tagged)

        #convert back to a string
        new_text = " ".join(lemmas)
        #replace textfield
        df[corpus_mapping.TEXT_FIELD][i] = new_text

    return df

